import os
import json
import re
from openai import OpenAI
from typing import List, Dict, Any


class LaborLawGuidance:
    """åŠ³åŠ¨æ³•ç»´æƒä¸¾è¯æŒ‡å¯¼ç³»ç»Ÿ"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç³»ç»Ÿ"""
        self.client = OpenAI(
            api_key=os.getenv("DASHSCOPE_API_KEY"),
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        )
        self.conversation_history = []
        self.user_evidence = {}
        self.required_evidence = []
        
    def load_conversation_history(self, file_path: str) -> bool:
        """åŠ è½½å¯¹è¯å†å²æ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if isinstance(data, list) and len(data) > 0:
                    self.conversation_history = data[0].get('conversations', [])
                    return True
                return False
        except Exception as e:
            print(f"åŠ è½½å¯¹è¯å†å²å¤±è´¥: {e}")
            return False
    
    def analyze_case_with_ai(self, conversation_data: List[Dict]) -> str:
        """ä½¿ç”¨AIåˆ†æåŠ³åŠ¨äº‰è®®æ¡ˆä¾‹"""
        try:
            # æ„å»ºå¯¹è¯å†…å®¹
            conversation_text = ""
            for msg in conversation_data:
                role = "ç”¨æˆ·" if msg['from'] == 'human' else "å¾‹å¸ˆ"
                conversation_text += f"{role}: {msg['value']}\n\n"
            
            system_prompt = """
            ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„åŠ³åŠ¨æ³•å¾‹å¸ˆï¼Œè¯·åŸºäºä»¥ä¸‹åŠ³åŠ¨äº‰è®®å¯¹è¯å†å²ï¼Œåˆ†ææ¡ˆä¾‹å¹¶æä¾›ä»¥ä¸‹ä¿¡æ¯ï¼š
            
            1. æ¡ˆä¾‹ç±»å‹å’Œäº‰è®®ç„¦ç‚¹
            2. åŠ³åŠ¨è€…ç”³è¯·ä»²è£æˆ–è¯‰è®¼æ—¶éœ€è¦å‡†å¤‡çš„å…·ä½“è¯æ®ææ–™æ¸…å•
            3. æ¯ç±»è¯æ®çš„æ³•å¾‹è¦ä»¶å’Œè¯æ˜æ ‡å‡†
            4. è¯æ®çš„é‡è¦æ€§ç­‰çº§ï¼ˆå…³é”®è¯æ®/é‡è¦è¯æ®/è¾…åŠ©è¯æ®ï¼‰
            
            è¯·ä»¥ç»“æ„åŒ–çš„æ–¹å¼å›ç­”ï¼Œä¾¿äºåç»­çš„äº¤äº’å¼æŒ‡å¯¼ã€‚
            """
            
            completion = self.client.chat.completions.create(
                model="qwen-max-latest",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"è¯·åˆ†æä»¥ä¸‹åŠ³åŠ¨äº‰è®®å¯¹è¯ï¼š\n\n{conversation_text}"}
                ],
                temperature=0.3
            )
            
            return completion.choices[0].message.content
            
        except Exception as e:
            return f"AIåˆ†æå¤±è´¥: {e}"
    
    def extract_required_evidence(self, ai_analysis: str) -> List[Dict]:
        """ä»AIåˆ†æç»“æœä¸­æå–æ‰€éœ€è¯æ®æ¸…å•
        ç›®æ ‡ï¼šç¡®ä¿å°½å¯èƒ½è§£æå‡ºâ€œå…¨éƒ¨â€è¯æ®é¡¹ï¼Œè€Œä¸æ˜¯é€€å›å•ä¸€é»˜è®¤é¡¹ã€‚
        ç­–ç•¥ï¼š
        1) å…ˆè¯·æ±‚æ¨¡å‹â€œåªè¿”å›JSONæ•°ç»„â€ï¼Œå¹¶å°½é‡ç”¨response_formatå¼ºåˆ¶JSONï¼›
        2) è§£æè¿”å›æ–‡æœ¬ä¸­çš„JSONä»£ç å—æˆ–æ–¹æ‹¬å·ç‰‡æ®µï¼›
        3) å¦‚æœä»å¤±è´¥ï¼Œåˆ™ä»ai_analysisåŸå§‹åˆ†ææ–‡æœ¬ä¸­å›æº¯è§£æè¦ç‚¹æ¡ç›®ï¼Œæ„é€ ç»“æ„åŒ–æ¸…å•ã€‚
        """
        import re

        def _extract_json_from_text(text: str) -> str | None:
            # ä¼˜å…ˆæå–```json ... ```ä»£ç å—
            code_block = re.search(r"```json\s*(\[.*?\])\s*```",
                                   text, re.DOTALL | re.IGNORECASE)
            if code_block:
                return code_block.group(1).strip()
            # å…¶æ¬¡æå–``` ... ```ä¸­çš„æ•°ç»„
            code_block_generic = re.search(r"```\s*(\[.*?\])\s*```",
                                           text, re.DOTALL | re.IGNORECASE)
            if code_block_generic:
                return code_block_generic.group(1).strip()
            # æœ€åå°è¯•æˆªå–ç¬¬ä¸€ä¸ª'['åˆ°æœ€åä¸€ä¸ª']'ä¹‹é—´å†…å®¹
            start = text.find('[')
            end = text.rfind(']')
            if start != -1 and end != -1 and end > start:
                candidate = text[start:end+1].strip()
                return candidate
            return None

        try:
            # ä½¿ç”¨AIæå–ç»“æ„åŒ–çš„è¯æ®æ¸…å•ï¼ˆå¼ºçº¦æŸä»…è¿”å›JSONï¼‰
            system_prompt = (
                "ä½ æ˜¯èµ„æ·±åŠ³åŠ¨æ³•è¯æ®æ¸…å•è§£æå™¨ã€‚è¯·ä»è¾“å…¥çš„åˆ†ææ–‡æœ¬ä¸­æå–è¯æ®æ¸…å•ï¼Œ"
                "å¹¶ä¸”â€˜åªè¿”å›â€™ä¸€ä¸ªJSONæ•°ç»„ï¼Œä¸è¦ä»»ä½•å…¶ä»–æ–‡å­—ã€è§£é‡Šæˆ–Markdownã€‚"
                "æ•°ç»„å…ƒç´ å­—æ®µï¼ševidence_type, description, legal_requirements, importance, collection_methodã€‚"
                "importance å–å€¼é™å®šï¼š'å…³é”®è¯æ®' | 'é‡è¦è¯æ®' | 'è¾…åŠ©è¯æ®'ã€‚"
            )

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": ai_analysis}
            ]

            # å°è¯•ä½¿ç”¨response_formatå¼ºåˆ¶JSONï¼ˆè‹¥ä¸æ”¯æŒå°†æŠ›é”™ï¼Œè¿›å…¥fallbackï¼‰
            try:
                completion = self.client.chat.completions.create(
                    model="qwen-max-latest",
                    messages=messages,
                    temperature=0.1,
                    response_format={"type": "json_object"}  # æœŸæœ›è¿”å›ä¸€ä¸ªå¯¹è±¡æˆ–æ•°ç»„
                )
                result_text = completion.choices[0].message.content
            except Exception:
                completion = self.client.chat.completions.create(
                    model="qwen-max-latest",
                    messages=messages,
                    temperature=0.1
                )
                result_text = completion.choices[0].message.content

            # å…ˆç›´æ¥å°è¯•è§£æ
            try:
                parsed = json.loads(result_text)
                # æœ‰çš„æ¨¡å‹åœ¨json_objectä¸‹ä¼šè¿”å›å¯¹è±¡åŒ…è£¹æ•°ç»„ï¼Œæ¯”å¦‚{"items": [...]}ï¼Œåšä¸€æ¬¡å±•å¼€
                if isinstance(parsed, dict):
                    for k, v in parsed.items():
                        if isinstance(v, list):
                            parsed = v
                            break
                if isinstance(parsed, list):
                    return self._normalize_evidence_items(parsed)
            except Exception:
                pass

            # ä»æ–‡æœ¬ä¸­æå–JSONç‰‡æ®µå†è§£æ
            json_snippet = _extract_json_from_text(result_text)
            if json_snippet:
                try:
                    parsed = json.loads(json_snippet)
                    if isinstance(parsed, list):
                        return self._normalize_evidence_items(parsed)
                except Exception:
                    pass

            # å…œåº•ï¼šç›´æ¥ä»åŸå§‹åˆ†ææ–‡æœ¬ä¸­è§£æï¼ˆé€šå¸¸ä¸ºMarkdownè¦ç‚¹åˆ—è¡¨ï¼‰
            fallback_items = self._fallback_parse_evidence_from_text(ai_analysis)
            if fallback_items:
                return self._normalize_evidence_items(fallback_items)

            # ä»å¤±è´¥ï¼Œä¿åº•è¿”å›å¤šé¡¹å¸¸è§è¯æ®è€Œéå•é¡¹
            return self._normalize_evidence_items([
                {
                    "evidence_type": "åŠ³åŠ¨åˆåŒ",
                    "description": "è¯æ˜åŠ³åŠ¨å…³ç³»å­˜åœ¨çš„åŸºç¡€æ–‡ä»¶",
                    "legal_requirements": "éœ€åŒæ–¹ç­¾å­—ç›–ç« ã€LOADED/ç­¾ç½²ï¼ŒçœŸå®æ€§ã€åˆæ³•æ€§ã€å…³è”æ€§",
                    "importance": "å…³é”®è¯æ®",
                    "collection_method": "ä¿ç•™åŸä»¶ä¸æ¸…æ™°å¤å°/æ‰«æä»¶ï¼›é‡ç‚¹é¡µæ‹ç…§å¤‡ä»½"
                },
                {
                    "evidence_type": "è§£é™¤åŠ³åŠ¨åˆåŒé€šçŸ¥ä¹¦",
                    "description": "è¯æ˜è§£é™¤äº‹å®ä¸ç†ç”±çš„æ ¸å¿ƒæ–‡ä»¶",
                    "legal_requirements": "åº”LOADEDè§£é™¤ä¾æ®ã€ç†ç”±ã€æ—¥æœŸå¹¶åŠ ç›–å…¬å¸å…¬ç« ï¼›ä¿ç•™é€è¾¾å‡­è¯",
                    "importance": "å…³é”®è¯æ®",
                    "collection_method": "ä¿ç•™åŸä»¶/é‚®å¯„å‡­è¯ï¼›å¦‚ä¸ºé‚®ä»¶/ç³»ç»Ÿé€šçŸ¥ï¼Œä¿ç•™å®Œæ•´æˆªå›¾ä¸å…ƒæ•°æ®"
                },
                {
                    "evidence_type": "å·¥èµ„å‘æ”¾è®°å½•",
                    "description": "è¯æ˜å·¥èµ„æ ‡å‡†ä¸å·²å‘æ”¾æƒ…å†µ",
                    "legal_requirements": "é“¶è¡Œæµæ°´/å·¥èµ„æ¡ä¸æœŸé—´ä¸€è‡´ï¼Œèƒ½å¤Ÿå¯¹åº”è‡³ä¸ªäººè´¦æˆ·åŠå‘è–ªä¸»ä½“",
                    "importance": "é‡è¦è¯æ®",
                    "collection_method": "ä¸‹è½½é“¶è¡Œæµæ°´ã€ä¿å­˜å·¥èµ„æ¡/é‚®ä»¶ï¼Œå¿…è¦æ—¶å‘è´¢åŠ¡ç´¢å–ç›–ç« è¯æ˜"
                },
                {
                    "evidence_type": "ç¤¾ä¿ç¼´çº³è®°å½•",
                    "description": "è¾…åŠ©è¯æ˜åŠ³åŠ¨å…³ç³»ä¸ç”¨å·¥ä¸»ä½“",
                    "legal_requirements": "ç¤¾ä¿ç¼´è´¹æ˜ç»†ä¸ä»»èŒæœŸé—´å¯¹åº”ï¼Œæ˜¾ç¤ºå•ä½åç§°ä¸ç¼´è´¹åŸºæ•°",
                    "importance": "é‡è¦è¯æ®",
                    "collection_method": "äººç¤¾App/çº¿ä¸‹å¤§å…æ‰“å°ç¼´è´¹æ˜ç»†ï¼Œä¿ç•™ç”µå­ä¸çº¸è´¨ç‰ˆ"
                },
                {
                    "evidence_type": "ç»©æ•ˆè€ƒæ ¸è®°å½•",
                    "description": "åé©³â€œä¸èƒ½èƒœä»»â€æˆ–è¯æ˜ç»©æ•ˆæ°´å¹³",
                    "legal_requirements": "æ¥æºå®¢è§‚ã€å½¢æˆäºäº‰è®®å‰ï¼Œèƒ½å¯¹åº”æœŸé—´ä¸å²—ä½",
                    "importance": "é‡è¦è¯æ®",
                    "collection_method": "å¯¼å‡ºç³»ç»Ÿè®°å½•ã€ä¿å­˜é‚®ä»¶ä¸æˆªå›¾ï¼Œæ ‡æ³¨æ—¥æœŸä¸æ¥æº"
                }
            ])

        except Exception as e:
            print(f"æå–è¯æ®æ¸…å•å¤±è´¥: {e}")
            return []

    # è¾…åŠ©ï¼šä»è‡ªç„¶è¯­è¨€/Markdownåˆ†ææ–‡æœ¬ä¸­å›æº¯è§£æè¯æ®é¡¹
    def _fallback_parse_evidence_from_text(self, text: str) -> List[Dict]:
        import re
        items: List[Dict] = []

        # é€šè¿‡å¸¸è§æ¨¡å¼æå–å½¢å¦‚ â€œ- **è¯æ®å**ï¼šæè¿°â€ çš„æ¡ç›®
        pattern = re.compile(r"^\s*[-\u2022]\s*\*\*(.+?)\*\*\s*[:ï¼š]\s*(.+?)\s*$",
                             re.MULTILINE)
        matches = pattern.findall(text)
        for name, desc in matches:
            items.append({
                "evidence_type": name.strip(),
                "description": desc.strip(),
            })

        # å¦‚æœªåŒ¹é…ï¼Œé€€è€Œæ±‚å…¶æ¬¡ï¼ŒåŒ¹é… â€œ- è¯æ®åï¼šæè¿°â€
        if not items:
            pattern2 = re.compile(r"^\s*[-\u2022]\s*(.+?)\s*[:ï¼š]\s*(.+?)\s*$",
                                  re.MULTILINE)
            for name, desc in pattern2.findall(text):
                # æ’é™¤å°èŠ‚æ ‡é¢˜ç­‰éè¯æ®è¡Œ
                if len(name) > 20:  # ç²—ç•¥è¿‡æ»¤ï¼šè¯æ®åä¸€èˆ¬ä¸å¤ªé•¿
                    continue
                items.append({
                    "evidence_type": name.strip().strip('ã€Šã€‹'),
                    "description": desc.strip(),
                })

        # ä¸ºæ¯é¡¹è¡¥é½é»˜è®¤å­—æ®µï¼ˆimportanceç­‰åç»­ç»Ÿä¸€å½’ä¸€ï¼‰
        return items

    # è¾…åŠ©ï¼šè§„èŒƒåŒ–ã€è¡¥å…¨è¯æ®é¡¹çš„å­—æ®µ
    def _normalize_evidence_items(self, items: List[Dict]) -> List[Dict]:
        def default_by_type(name: str) -> Dict[str, str]:
            name = name.strip().strip('ã€Šã€‹')
            mapping = {
                "åŠ³åŠ¨åˆåŒ": {
                    "importance": "å…³é”®è¯æ®",
                    "legal_requirements": "éœ€åŒæ–¹ç­¾å­—ç›–ç« ã€æ¡æ¬¾å®Œæ•´ï¼ŒçœŸå®æ€§ã€åˆæ³•æ€§ã€å…³è”æ€§",
                    "collection_method": "ä¿ç•™åŸä»¶ä¸å¤å°ä»¶ï¼Œå…³é”®é¡µæ‹ç…§ç•™å­˜"
                },
                "è§£é™¤åŠ³åŠ¨åˆåŒé€šçŸ¥ä¹¦": {
                    "importance": "å…³é”®è¯æ®",
                    "legal_requirements": "å†™æ˜ç†ç”±/ä¾æ®/æ—¥æœŸå¹¶åŠ ç›–å…¬ç« ï¼Œä¿ç•™é€è¾¾å‡­è¯",
                    "collection_method": "ä¿ç•™åŸä»¶/æˆªå›¾ä¸é‚®ä»¶å¤´ä¿¡æ¯ï¼Œä¿å­˜é‚®å¯„å‡­è¯"
                },
                "ç¤¾ä¿ç¼´çº³è®°å½•": {
                    "importance": "é‡è¦è¯æ®",
                    "legal_requirements": "æ˜ç¤ºå•ä½åç§°ã€åŸºæ•°ä¸ç¼´è´¹æœŸé—´ï¼Œèƒ½å¯¹åº”ä»»èŒæ—¶æ®µ",
                    "collection_method": "äººç¤¾App/å¤§å…æ‰“å°ç¼´è´¹æ˜ç»†"
                },
                "å·¥èµ„å‘æ”¾è®°å½•": {
                    "importance": "é‡è¦è¯æ®",
                    "legal_requirements": "é“¶è¡Œæµæ°´ä¸å‘è–ªè®°å½•ä¸€è‡´ï¼Œèƒ½å¯¹åº”ä¸ªäººè´¦æˆ·ä¸å‘è–ªä¸»ä½“",
                    "collection_method": "ä¸‹è½½æµæ°´/ä¿å­˜å·¥èµ„æ¡ï¼Œå¿…è¦æ—¶å¼€å…·æ”¶å…¥è¯æ˜"
                },
                "ç»©æ•ˆè€ƒæ ¸è®°å½•": {
                    "importance": "é‡è¦è¯æ®",
                    "legal_requirements": "æ¥æºå®¢è§‚ã€å½¢æˆäºäº‰è®®å‰ï¼Œèƒ½å¯¹åº”æœŸé—´ä¸å²—ä½",
                    "collection_method": "å¯¼å‡ºç³»ç»Ÿè®°å½•ã€ä¿å­˜é‚®ä»¶ä¸æˆªå›¾"
                },
                "åŸ¹è®­æˆ–è°ƒå²—è®°å½•": {
                    "importance": "é‡è¦è¯æ®",
                    "legal_requirements": "LOADEDåŸ¹è®­/è°ƒå²—æ—¶é—´ã€åŸå› ã€å²—ä½åŠç¡®è®¤æ–¹å¼",
                    "collection_method": "ä¿ç•™OA/é‚®ä»¶/é€šçŸ¥æˆªå›¾ï¼Œå‘HRç´¢å–ç›¸å…³è®°å½•"
                },
                "å…¥èŒè¯æ˜": {
                    "importance": "è¾…åŠ©è¯æ®",
                    "legal_requirements": "èƒ½åæ˜ å…¥èŒæ—¥æœŸä¸å²—ä½ä¿¡æ¯",
                    "collection_method": "ç”¨äººå•ä½å¼€å…·ï¼Œæˆ–ä»¥åˆåŒé¦–é¡µ/ç™»è®°è¡¨æ›¿ä»£"
                },
                "æœˆå·¥èµ„è¯æ˜": {
                    "importance": "é‡è¦è¯æ®",
                    "legal_requirements": "èƒ½åæ˜ æœ€è¿‘12ä¸ªæœˆå¹³å‡å·¥èµ„åŠæ„æˆ",
                    "collection_method": "é“¶è¡Œæµæ°´+å·¥èµ„æ¡/HRç›–ç« è¯æ˜"
                },
                "å¹´å‡æ”¿ç­–æ–‡ä»¶": {
                    "importance": "è¾…åŠ©è¯æ®",
                    "legal_requirements": "å…¬å¸æ­£å¼åˆ¶åº¦/å‘˜å·¥æ‰‹å†Œç”Ÿæ•ˆå¹¶å…¬ç¤º",
                    "collection_method": "ä¸‹è½½åˆ¶åº¦/æ‰‹å†ŒPDFæˆ–ç›–ç« çº¸è´¨ç‰ˆ"
                },
                "æœªä¼‘å¹´å‡è®°å½•": {
                    "importance": "é‡è¦è¯æ®",
                    "legal_requirements": "èƒ½åæ˜ æœªä¼‘å¤©æ•°ã€æœŸé—´ä¸å®¡æ‰¹çŠ¶æ€",
                    "collection_method": "ç³»ç»Ÿæˆªå›¾/è€ƒå‹¤å¯¼å‡ºï¼Œé‚®ä»¶ç¡®è®¤"
                },
                "èŠå¤©è®°å½•": {
                    "importance": "è¾…åŠ©è¯æ®",
                    "legal_requirements": "æ¥æºçœŸå®ã€æœªç¯¡æ”¹ï¼Œèƒ½åæ˜ æ²Ÿé€šäº‹å®",
                    "collection_method": "å¯¼å‡ºå¾®ä¿¡/ä¼å¾®èŠå¤©ï¼Œä¿ç•™åŸæ–‡ä»¶ä¸æ—¶é—´æˆ³"
                },
                "å…¬å¸å†…éƒ¨æ–‡ä»¶": {
                    "importance": "è¾…åŠ©è¯æ®",
                    "legal_requirements": "ä¸å²—ä½/è€ƒæ ¸/åˆ¶åº¦ç›´æ¥ç›¸å…³ï¼Œæ¥æºå¯è¿½æº¯",
                    "collection_method": "ä¿å­˜å²—ä½è¯´æ˜ä¹¦ã€è€ƒæ ¸æ ‡å‡†ç­‰ï¼Œå¹¶æ³¨æ˜æ¥æº"
                },
            }
            base = mapping.get(name, {
                "importance": "é‡è¦è¯æ®",
                "legal_requirements": "æ»¡è¶³çœŸå®æ€§ã€åˆæ³•æ€§ã€å…³è”æ€§ä¸‰æ€§ï¼Œæ³¨æ„å½¢æˆæ—¶é—´ä¸æ¥æº",
                "collection_method": "ä¿ç•™åŸä»¶/æˆªå›¾ä¸ç”µå­ç‰ˆå¤‡ä»½ï¼Œå¿…è¦æ—¶å‘å•ä½ç”³è¯·è¯æ˜"
            })
            return base

        normalized: List[Dict] = []
        for raw in items:
            name = (raw.get("evidence_type") or raw.get("name") or "").strip()
            if not name:
                # æ²¡æœ‰åç§°åˆ™è·³è¿‡
                continue
            defaults = default_by_type(name)
            normalized.append({
                "evidence_type": name.strip().strip('ã€Šã€‹'),
                "description": raw.get("description") or raw.get("desc") or "",
                "legal_requirements": raw.get("legal_requirements") or defaults["legal_requirements"],
                "importance": raw.get("importance") or defaults["importance"],
                "collection_method": raw.get("collection_method") or defaults["collection_method"],
            })

        # å»é‡ï¼ˆæŒ‰ evidence_typeï¼‰å¹¶ä¿æŒé¡ºåº
        seen = set()
        deduped: List[Dict] = []
        for it in normalized:
            key = it["evidence_type"]
            if key in seen:
                continue
            seen.add(key)
            deduped.append(it)
        return deduped


    def interactive_evidence_check(self, evidence_list: List[Dict]) -> Dict:
        """äº¤äº’å¼è¯æ®æ ¸æŸ¥ - ä¸“ä¸šåŒ–ä¸¤è½®å¾‹å¸ˆå¯¹è¯æµç¨‹"""
        
        # ç¬¬ä¸€è½®å¯¹è¯ï¼šå¾‹å¸ˆåˆ—å‡ºè¯æ®æ¸…å•å¹¶è¯¢é—®ç”¨æˆ·æŒæœ‰æƒ…å†µ
        print("\n=== å¾‹å¸ˆè¯æ®æŒ‡å¯¼ ===")
        print("\nå¾‹å¸ˆï¼šæ ¹æ®æ¡ˆæƒ…åˆ†æï¼Œæ‚¨éœ€è¦å‡†å¤‡ä»¥ä¸‹å…³é”®è¯æ®ï¼š\n")
        
        # è¯¦ç»†åˆ—å‡ºæ‰€éœ€è¯æ®æ¸…å•
        for i, evidence in enumerate(evidence_list, 1):
            importance_icon = "ğŸ”´" if evidence['importance'] == 'å…³é”®è¯æ®' else "ğŸŸ¡" if evidence['importance'] == 'é‡è¦è¯æ®' else "ğŸŸ¢"
            print(f"{i}. {importance_icon} {evidence['evidence_type']} ({evidence['importance']})")
            print(f"   ä½œç”¨ï¼š{evidence['description']}")
            print(f"   æ³•å¾‹è¦ä»¶ï¼š{evidence['legal_requirements']}")
            print()
        
        print("å¾‹å¸ˆï¼šè¯·é—®æ‚¨ç›®å‰æ‰‹ä¸Šæœ‰å“ªäº›è¯æ®ææ–™ï¼Ÿ")
        print("ï¼ˆè¯·ç›´æ¥è¾“å…¥æ‚¨æŒæœ‰çš„è¯æ®ææ–™ï¼Œä¾‹å¦‚ï¼šæˆ‘ç›®å‰æŒæœ‰ä¹¦é¢åŠ³åŠ¨åˆåŒã€è§£é™¤åŠ³åŠ¨åˆåŒé€šçŸ¥ä¹¦ï¼‰")
        
        # ç”¨æˆ·è‡ªç”±è¾“å…¥æŒæœ‰çš„è¯æ®
        user_input = input("\næ‚¨çš„å›ç­”ï¼š").strip()
        
        # è§£æç”¨æˆ·è¾“å…¥ï¼ŒåŒ¹é…è¯æ®ç±»å‹
        user_evidence = self._parse_user_evidence_input(user_input, evidence_list)
        
        # ç¬¬äºŒè½®å¯¹è¯ï¼šå¾‹å¸ˆç¡®è®¤å¹¶åˆ†æç°æœ‰è¯æ®ï¼Œæä¾›ç¼ºå¤±è¯æ®çš„å–è¯å»ºè®®
        print("\n" + "=" * 60)
        print("\nå¾‹å¸ˆï¼šå·²ç¡®è®¤æ‚¨ç°æœ‰çš„è¯æ®ææ–™ã€‚è®©æˆ‘ä¸ºæ‚¨è¿›è¡Œä¸“ä¸šåˆ†æï¼š\n")
        
        # ç¡®è®¤ç”¨æˆ·ç°æœ‰è¯æ®
        owned_evidence = [k for k, v in user_evidence.items() if v['status'] in ['æ˜¯', 'éƒ¨åˆ†']]
        if owned_evidence:
            print("âœ… æ‚¨ç›®å‰æŒæœ‰çš„è¯æ®ï¼š")
            for evidence_type in owned_evidence:
                status_text = "å®Œæ•´" if user_evidence[evidence_type]['status'] == 'æ˜¯' else "éƒ¨åˆ†"
                print(f"   â€¢ {evidence_type} ({status_text})")
            print()
        
        # é’ˆå¯¹ç°æœ‰è¯æ®è¿›è¡Œå…³é”®æ¡æ¬¾åˆ†æ
        if owned_evidence:
            print("ğŸ“‹ é’ˆå¯¹è¿™äº›ææ–™ï¼Œéœ€è¦é‡ç‚¹å…³æ³¨ï¼š")
            for evidence_type in owned_evidence:
                evidence_info = user_evidence[evidence_type]['evidence_info']
                analysis = self._analyze_evidence_key_points(evidence_type, evidence_info)
                print(f"\nâ€¢ {evidence_type}ä¸­çš„å…³é”®è¦ç‚¹ï¼š")
                print(f"  {analysis}")
        
        # å¯¹äºç¼ºå¤±çš„è¯æ®ï¼Œæä¾›å…·ä½“å–è¯æ–¹æ³•
        missing_evidence = [evidence for evidence in evidence_list 
                          if evidence['evidence_type'] not in user_evidence 
                          or user_evidence[evidence['evidence_type']]['status'] == 'å¦']
        
        if missing_evidence:
            print("\nâš ï¸  å¯¹äºç¼ºå¤±çš„è¯æ®ï¼Œå»ºè®®é€šè¿‡ä»¥ä¸‹æ–¹å¼æ”¶é›†ï¼š")
            for evidence in missing_evidence:
                if evidence['importance'] == 'å…³é”®è¯æ®':
                    print(f"\nğŸ”´ {evidence['evidence_type']} (å…³é”®è¯æ® - ä¼˜å…ˆæ”¶é›†)")
                    print(f"   å–è¯æ–¹æ³•ï¼š{evidence['collection_method']}")
                    print(f"   é‡è¦æ€§ï¼š{evidence['description']}")
            
            for evidence in missing_evidence:
                if evidence['importance'] in ['é‡è¦è¯æ®', 'è¾…åŠ©è¯æ®']:
                    icon = "ğŸŸ¡" if evidence['importance'] == 'é‡è¦è¯æ®' else "ğŸŸ¢"
                    print(f"\n{icon} {evidence['evidence_type']} ({evidence['importance']})")
                    print(f"   å–è¯æ–¹æ³•ï¼š{evidence['collection_method']}")
        
        print("\nå¾‹å¸ˆï¼šä»¥ä¸Šæ˜¯åŸºäºæ‚¨æ¡ˆä»¶æƒ…å†µçš„ä¸“ä¸šå»ºè®®ï¼Œå»ºè®®ä¼˜å…ˆæ”¶é›†å…³é”®è¯æ®ä»¥æé«˜ç»´æƒæˆåŠŸç‡ã€‚")
        
        return user_evidence
    
    def _parse_user_evidence_input(self, user_input: str, evidence_list: List[Dict]) -> Dict:
        """è§£æç”¨æˆ·è¾“å…¥çš„è¯æ®ææ–™ï¼Œä»…ä»ç”¨æˆ·è¾“å…¥ä¸­æå–å…¶â€œå·²æŒæœ‰/éƒ¨åˆ†æŒæœ‰â€çš„è¯æ®ã€‚
        - ä»…è¿”å›ç”¨æˆ·å£°ç§°æŒæœ‰ï¼ˆå®Œæ•´æˆ–éƒ¨åˆ†ï¼‰çš„è¯æ®é¡¹ï¼›ä¸ä¸ºæœªæåŠæˆ–æ˜ç¡®å¦å®šçš„è¯æ®å¡«å……â€œå¦â€ï¼Œ
          ä»¥ä¾¿åç»­é€šè¿‡â€œæœªåœ¨å­—å…¸ä¸­â€åˆ¤å®šä¸ºç¼ºå¤±å¹¶æä¾›å–è¯å»ºè®®ã€‚
        - å…·å¤‡æ›´ç¨³å¥çš„å¦å®šã€éƒ¨åˆ†ä¸è‚¯å®šè¯†åˆ«ï¼Œé¿å…â€œæ²¡æœ‰åŠ³åŠ¨åˆåŒâ€è¢«è¯¯åˆ¤ä¸ºæŒæœ‰ã€‚
        """
        result: Dict[str, Dict] = {}
        text = (user_input or "").strip()
        if not text:
            return result

        # å°†è¾“å…¥æ‹†åˆ†ä¸ºè‹¥å¹²è¯­å¥ï¼Œä¾¿äºå°±è¿‘åˆ¤æ–­å¦å®š/éƒ¨åˆ†/è‚¯å®šè¯­ä¹‰
        seps = "ï¼Œ,ã€‚.;ï¼›!ï¼?ï¼Ÿ\n"
        sentences: List[str] = []
        buf = ""
        for ch in text:
            buf += ch
            if ch in seps:
                sentences.append(buf.strip())
                buf = ""
        if buf.strip():
            sentences.append(buf.strip())

        negative_markers = ["æ²¡æœ‰", "æ²¡", "æœª", "æ— ", "ç¼º", "ä¸åœ¨æ‰‹ä¸Š", "æ²¡å¸¦", "æœªæ‹¿åˆ°", "æ²¡æ‹¿åˆ°", "æœªæ”¶åˆ°", "æ²¡æ”¶åˆ°", "æ‰¾ä¸åˆ°", "ä¸¢äº†", "æœªç­¾", "æ²¡ç­¾"]
        partial_markers = ["éƒ¨åˆ†", "ä¸å®Œæ•´", "ç¼ºå°‘", "åªæœ‰", "å¤å°ä»¶", "ç”µå­ç‰ˆ", "æˆªå›¾", "å½±å°ä»¶", "ç¼ºé¡µ", "ä»…æœ‰", "ç…§ç‰‡", "éƒ¨åˆ†æœˆä»½", "éƒ¨åˆ†è®°å½•"]
        positive_markers = ["æœ‰", "æŒæœ‰", "åœ¨æ‰‹ä¸Š", "æ‹¿åˆ°", "æ”¶åˆ°", "ä¿å­˜", "ç•™å­˜", "å…·å¤‡", "å·²ç»", "å·²", "ç°æœ‰", "æ‰‹é‡Œæœ‰", "æ‰‹ä¸Šæœ‰", "å¯ä»¥æä¾›"]

        # å¸¸è§è¯æ®åˆ«åæ˜ å°„ï¼ˆåœ¨ä¸æ”¹å˜ evidence_list çš„å‰æä¸‹å¢å¼ºåŒ¹é…èƒ½åŠ›ï¼‰
        alias_map: Dict[str, List[str]] = {
            "åŠ³åŠ¨åˆåŒ": ["ä¹¦é¢åŠ³åŠ¨åˆåŒ", "åŠ³åŠ¨åˆåŒä¹¦", "åˆåŒ", "åŠ³åŠ¨åè®®", "è˜ç”¨åˆåŒ", "å…¥èŒåˆåŒ"],
            "è§£é™¤åŠ³åŠ¨åˆåŒé€šçŸ¥ä¹¦": ["è§£é›‡é€šçŸ¥", "è§£é™¤é€šçŸ¥", "è¾é€€é€šçŸ¥", "è§£é™¤åŠ³åŠ¨åˆåŒé€šçŸ¥", "è§£è˜é€šçŸ¥", "å¼€é™¤é€šçŸ¥"],
            "å·¥èµ„æ¡": ["å·¥èµ„å‘æ”¾è®°å½•", "è–ªèµ„æ¡", "å·¥èµ„å•", "è–ªèµ„å•", "å‘è–ªè®°å½•", "è–ªé…¬è®°å½•", "å·¥èµ„æ¡æˆªå›¾"],
            "ç¤¾ä¿ç¼´çº³è®°å½•": ["ç¤¾ä¿è®°å½•", "ç¤¾ä¿ç¼´è´¹è®°å½•", "ç¤¾ä¿æ˜ç»†", "ç¤¾ä¿æ¸…å•", "äº”é™©ç¼´è´¹è®°å½•", "å‚ä¿è®°å½•"],
            "è€ƒå‹¤è®°å½•": ["æ‰“å¡è®°å½•", "é—¨ç¦è®°å½•", "æ’ç­è®°å½•", "å‡ºå‹¤è®°å½•", "å·¥æ—¶è®°å½•", "åŠ ç­è®°å½•"],
            "åŸ¹è®­æˆ–è°ƒå²—è®°å½•": ["åŸ¹è®­è®°å½•", "åŸ¹è®­è¯æ˜", "è°ƒå²—é€šçŸ¥", "å²—ä½è°ƒæ•´è®°å½•", "å²—ä½å˜æ›´è®°å½•", "è°ƒå²—å‡½"],
            "æœªä¼‘å¹´å‡è®°å½•": ["å¹´å‡è®°å½•", "å¹´ä¼‘å‡è®°å½•", "å¸¦è–ªå¹´å‡è®°å½•", "å¹´å‡ä½™é¢", "å‡æœŸè®°å½•"],
        }

        def gen_aliases(name: str) -> List[str]:
            # åŸºäºåŸåç§°ç”Ÿæˆä¸€äº›ç®€åŒ–åˆ«å
            base = [name]
            simplified = name
            for suf in ["ä¹¦", "é€šçŸ¥ä¹¦", "è¯æ˜", "è®°å½•", "ææ–™", "æ¸…å•", "åˆåŒä¹¦", "åè®®ä¹¦", "è¯´æ˜"]:
                simplified = simplified.replace(suf, "")
            simplified = simplified.strip()
            if simplified and simplified != name:
                base.append(simplified)
            # åˆå¹¶é¢„ç½®åˆ«å
            if name in alias_map:
                base.extend(alias_map[name])
            # å»é‡å¹¶æŒ‰é•¿åº¦é™åºï¼Œä¼˜å…ˆåŒ¹é…æ›´é•¿æ›´å…·ä½“çš„åˆ«å
            dedup = []
            seen = set()
            for a in base:
                a = a.strip()
                if a and a not in seen:
                    seen.add(a)
                    dedup.append(a)
            dedup.sort(key=len, reverse=True)
            return dedup

        def sentence_has_marker(sent: str, markers: List[str]) -> bool:
            return any(m in sent for m in markers)

        for evidence in evidence_list:
            etype = evidence.get("evidence_type", "").strip()
            if not etype:
                continue
            aliases = gen_aliases(etype)
            status_for_item = None  # None/"æ˜¯"/"éƒ¨åˆ†"
            matched_sentence = None

            for sent in sentences:
                # å¦‚æœè¯¥å¥æœªæ¶‰åŠä»»ä½•åˆ«ååˆ™è·³è¿‡
                alias_hit = None
                for al in aliases:
                    if al and al in sent:
                        alias_hit = al
                        break
                if not alias_hit:
                    continue

                # è‹¥å‡ºç°æ˜ç¡®å¦å®šï¼Œä¸”æ— æ˜æ˜¾è‚¯å®šï¼Œè§†ä¸ºæœªæŒæœ‰ï¼ˆä¸è®°å½•åˆ°ç»“æœä¸­ï¼‰
                neg = sentence_has_marker(sent, negative_markers)
                pos = sentence_has_marker(sent, positive_markers)
                part = sentence_has_marker(sent, partial_markers)

                # æ›´ç»†çš„å°±è¿‘å¦å®šåˆ¤æ–­ï¼šåˆ«åå‰ 6 ä¸ªå­—ç¬¦å†…è‹¥å‡ºç°å¦å®šè¯ï¼Œä¹Ÿè§†ä¸ºå¦å®š
                try:
                    idx = sent.index(alias_hit)
                    window = sent[max(0, idx-6):idx]
                    if any(m in window for m in negative_markers):
                        neg = True
                except ValueError:
                    pass

                if neg and not pos:
                    # æ˜ç¡®è¡¨ç¤ºæ²¡æœ‰è¯¥è¯æ® -> ä¸çº³å…¥å·²æŒæœ‰æ¸…å•
                    continue

                # æœ‰è‚¯å®šæˆ–æœªå¦å®šä¸”è¢«æåŠï¼Œç»“åˆæ˜¯å¦éƒ¨åˆ†çš„æè¿°
                if part:
                    status_for_item = "éƒ¨åˆ†"
                else:
                    status_for_item = "æ˜¯"
                matched_sentence = sent
                break  # è¯¥è¯æ®å·²å½’ç±»ï¼Œæ— éœ€å†çœ‹å…¶ä»–å¥å­

            if status_for_item:
                result[etype] = {
                    "status": status_for_item,
                    "evidence_info": evidence,
                    "details": f"ä»ç”¨æˆ·è¾“å…¥ä¸­è¯†åˆ«ï¼š{matched_sentence or ''}".strip()
                }

        return result
    
    def _analyze_evidence_key_points(self, evidence_type: str, evidence_info: Dict) -> str:
        """åˆ†æè¯æ®çš„å…³é”®è¦ç‚¹"""
        try:
            system_prompt = f"""
            ä½ æ˜¯ä¸“ä¸šçš„åŠ³åŠ¨æ³•å¾‹å¸ˆï¼Œè¯·é’ˆå¯¹{evidence_type}è¿™ç±»è¯æ®ï¼Œåˆ†æå…¶å…³é”®æ³•å¾‹è¦ç‚¹ã€‚
            
            è¯æ®ä¿¡æ¯ï¼š{evidence_info}
            
            è¯·ç®€è¦è¯´æ˜åœ¨å®¡æŸ¥è¿™ç±»è¯æ®æ—¶éœ€è¦é‡ç‚¹å…³æ³¨çš„æ¡æ¬¾æˆ–è¦ç‚¹ï¼Œ
            ä»¥åŠè¿™äº›è¦ç‚¹å¯¹æ¡ˆä»¶çš„é‡è¦æ„ä¹‰ã€‚å›ç­”è¦ä¸“ä¸šä½†é€šä¿—æ˜“æ‡‚ï¼Œä¸è¶…è¿‡100å­—ã€‚
            """
            
            completion = self.client.chat.completions.create(
                model="qwen-max-latest",
                messages=[
                    {"role": "system", "content": system_prompt}
                ],
                temperature=0.2
            )
            
            return completion.choices[0].message.content
            
        except Exception as e:
            # æä¾›é»˜è®¤çš„å…³é”®è¦ç‚¹åˆ†æ
            default_analysis = {
                'åŠ³åŠ¨åˆåŒ': 'é‡ç‚¹å…³æ³¨å·¥ä½œå²—ä½ã€å·¥èµ„æ ‡å‡†ã€å·¥ä½œæ—¶é—´ã€åˆåŒæœŸé™ç­‰æ¡æ¬¾æ˜¯å¦æ˜ç¡®ï¼Œä»¥åŠåŒæ–¹ç­¾å­—ç›–ç« æ˜¯å¦å®Œæ•´',
                'è§£é™¤åŠ³åŠ¨åˆåŒé€šçŸ¥ä¹¦': 'é‡ç‚¹å…³æ³¨è§£é™¤ç†ç”±æ˜¯å¦åˆæ³•ã€ç¨‹åºæ˜¯å¦è§„èŒƒã€æ˜¯å¦æåŠç»æµè¡¥å¿ç­‰å…³é”®ä¿¡æ¯',
                'å·¥èµ„æ¡': 'é‡ç‚¹å…³æ³¨å·¥èµ„æ„æˆã€å‘æ”¾æ—¶é—´ã€æ‰£æ¬¾é¡¹ç›®æ˜¯å¦åˆç†ï¼Œä»¥åŠæ˜¯å¦èƒ½è¯æ˜å®é™…å·¥èµ„æ°´å¹³',
                'è€ƒå‹¤è®°å½•': 'é‡ç‚¹å…³æ³¨å·¥ä½œæ—¶é—´ã€åŠ ç­æƒ…å†µã€è¯·å‡è®°å½•æ˜¯å¦çœŸå®å®Œæ•´ï¼Œèƒ½å¦è¯æ˜å®é™…å·¥ä½œçŠ¶å†µ'
            }
            return default_analysis.get(evidence_type, f'é‡ç‚¹å…³æ³¨{evidence_type}çš„çœŸå®æ€§ã€å®Œæ•´æ€§å’Œæ³•å¾‹æ•ˆåŠ›')
    
    def provide_collection_guidance(self, user_evidence: Dict, evidence_list: List[Dict]):
        """æä¾›å–è¯æŒ‡å¯¼"""
        print("\n=== å–è¯æŒ‡å¯¼å»ºè®® ===")
        
        missing_evidence = []
        incomplete_evidence = []
        
        for evidence in evidence_list:
            evidence_type = evidence['evidence_type']
            if evidence_type not in user_evidence:
                missing_evidence.append(evidence)
            elif user_evidence[evidence_type]['status'] in ['å¦', 'éƒ¨åˆ†']:
                incomplete_evidence.append(evidence)
        
        if missing_evidence:
            print("\nã€ç¼ºå¤±çš„å…³é”®è¯æ®ã€‘")
            for evidence in missing_evidence:
                if evidence['importance'] == 'å…³é”®è¯æ®':
                    print(f"\nâš ï¸  {evidence['evidence_type']} (å…³é”®è¯æ®)")
                    print(f"   ä½œç”¨: {evidence['description']}")
                    print(f"   å–è¯æ–¹æ³•: {evidence['collection_method']}")
                    print(f"   æ³•å¾‹è¦ä»¶: {evidence['legal_requirements']}")
        
        if incomplete_evidence:
            print("\nã€éœ€è¦å®Œå–„çš„è¯æ®ã€‘")
            for evidence in incomplete_evidence:
                print(f"\nğŸ“‹ {evidence['evidence_type']}")
                print(f"   å®Œå–„å»ºè®®: {evidence['collection_method']}")
        
        # æä¾›ä¸ªæ€§åŒ–å»ºè®®
        self.provide_personalized_advice(user_evidence, evidence_list)
    
    def provide_personalized_advice(self, user_evidence: Dict, evidence_list: List[Dict]):
        """æä¾›ä¸ªæ€§åŒ–å»ºè®®"""
        try:
            # æ„å»ºç”¨æˆ·è¯æ®æƒ…å†µæè¿°
            evidence_summary = ""
            for evidence_type, info in user_evidence.items():
                evidence_summary += f"{evidence_type}: {info['status']}"
                if 'details' in info:
                    evidence_summary += f" ({info['details']})"
                evidence_summary += "\n"
            
            system_prompt = """
            åŸºäºç”¨æˆ·å½“å‰çš„è¯æ®æŒæœ‰æƒ…å†µï¼Œè¯·æä¾›ä¸ªæ€§åŒ–çš„ç»´æƒå»ºè®®ï¼š
            1. ä¼˜å…ˆçº§æœ€é«˜çš„å–è¯ä»»åŠ¡
            2. æ³¨æ„äº‹é¡¹å’Œé£é™©æç¤º
            
            è¯·ç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€ï¼Œç»™å‡ºå®ç”¨çš„å»ºè®®ã€‚ï¼ˆä¸è¶…è¿‡200ä¸ªå­—ï¼‰
            """
            
            completion = self.client.chat.completions.create(
                model="qwen-max-latest",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"ç”¨æˆ·è¯æ®æƒ…å†µï¼š\n{evidence_summary}"}
                ],
                temperature=0.3
            )
            
            print("\n=== ä¸ªæ€§åŒ–ç»´æƒå»ºè®® ===")
            print(completion.choices[0].message.content)
            
        except Exception as e:
            print(f"\nç”Ÿæˆä¸ªæ€§åŒ–å»ºè®®å¤±è´¥: {e}")
    
    def run_guidance_session(self, conversation_file: str = "conversation.json"):
        """è¿è¡Œå®Œæ•´çš„æŒ‡å¯¼ä¼šè¯"""
        print("=" * 60)
        print("         åŠ³åŠ¨æ³•ç»´æƒä¸¾è¯æŒ‡å¯¼ç³»ç»Ÿ")
        print("=" * 60)
        
        # 1. åŠ è½½å¯¹è¯å†å²
        print("\næ­£åœ¨åŠ è½½æ¡ˆä¾‹æ•°æ®...")
        if not self.load_conversation_history(conversation_file):
            print("âŒ æ— æ³•åŠ è½½å¯¹è¯å†å²æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„")
            return
        
        print("âœ… æ¡ˆä¾‹æ•°æ®åŠ è½½æˆåŠŸ")
        
        # 2. AIåˆ†ææ¡ˆä¾‹
        print("\næ­£åœ¨åˆ†ææ¡ˆä¾‹...")
        ai_analysis = self.analyze_case_with_ai(self.conversation_history)
        print("\n=== æ¡ˆä¾‹åˆ†æç»“æœ ===")
        print(ai_analysis)
        
        # 3. æå–è¯æ®æ¸…å•
        print("\næ­£åœ¨ç”Ÿæˆè¯æ®æ¸…å•...")
        evidence_list = self.extract_required_evidence(ai_analysis)
        print(evidence_list)
        
        if not evidence_list:
            print("âŒ æ— æ³•ç”Ÿæˆè¯æ®æ¸…å•")
            return
        
        # 4. äº¤äº’å¼è¯æ®æ ¸æŸ¥
        user_evidence = self.interactive_evidence_check(evidence_list)
        
        # 5. æä¾›å–è¯æŒ‡å¯¼
        self.provide_collection_guidance(user_evidence, evidence_list)
        
        print("\n=== æŒ‡å¯¼ä¼šè¯ç»“æŸ ===")
        print("å¦‚éœ€è¿›ä¸€æ­¥å’¨è¯¢ï¼Œå»ºè®®è”ç³»ä¸“ä¸šå¾‹å¸ˆã€‚")


def labor_law_guidance_main(conversation_file: str = "conversation.json"):
    """åŠ³åŠ¨æ³•ç»´æƒä¸¾è¯æŒ‡å¯¼ä¸»å‡½æ•°
    
    Args:
        conversation_file: å¯¹è¯å†å²æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸ºå½“å‰ç›®å½•ä¸‹çš„conversation.json
    
    Returns:
        None
    
    åŠŸèƒ½è¯´æ˜:
        1. åŸºäºå¯¹è¯å†å²åˆ†æåŠ³åŠ¨äº‰è®®æ¡ˆä¾‹
        2. ç”Ÿæˆæ‰€éœ€è¯æ®ææ–™æ¸…å•
        3. äº¤äº’å¼è¯¢é—®ç”¨æˆ·å·²æœ‰è¯æ®
        4. è¯„ä¼°è¯æ®è´¨é‡å’Œæ³•å¾‹è¦ä»¶
        5. æä¾›ä¸ªæ€§åŒ–å–è¯å»ºè®®
    
    ä½¿ç”¨ç¤ºä¾‹:
        # ä½¿ç”¨é»˜è®¤å¯¹è¯æ–‡ä»¶
        labor_law_guidance_main()
        
        # æŒ‡å®šå¯¹è¯æ–‡ä»¶è·¯å¾„
        labor_law_guidance_main("path/to/your/conversation.json")
    """
    try:
        # æ£€æŸ¥ç¯å¢ƒå˜é‡
        if not os.getenv("DASHSCOPE_API_KEY"):
            print("âŒ è¯·è®¾ç½®DASHSCOPE_API_KEYç¯å¢ƒå˜é‡")
            print("   export DASHSCOPE_API_KEY=your_api_key")
            return
        
        # åˆ›å»ºæŒ‡å¯¼ç³»ç»Ÿå®ä¾‹
        guidance_system = LaborLawGuidance()
        
        # è¿è¡ŒæŒ‡å¯¼ä¼šè¯
        guidance_system.run_guidance_session(conversation_file)
        
    except KeyboardInterrupt:
        print("\n\nç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"\nâŒ ç³»ç»Ÿé”™è¯¯: {e}")
        print("è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé…ç½®")


if __name__ == "__main__":
    # ç›´æ¥è¿è¡Œæ—¶ä½¿ç”¨é»˜è®¤é…ç½®
    labor_law_guidance_main()